"""
Benchmark script for GalLoP on MuSciClaims dataset.
Supports:
1. Base Benchmark
2. Localized Only
3. GalLoP Only
4. Localized + GalLoP
"""

import os
import sys
import json
import torch
import argparse
import random
from tqdm import tqdm
from PIL import Image
from transformers import AutoProcessor, AutoModelForCausalLM
from peft import PeftModel

# Add parent directory to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from benchmark.data_loader import load_dataset_with_images, get_image_for_sample, PAPER_FIGURES_DIR
from benchmark.metrics import calculate_metrics, print_metrics

def load_model(args):
    """Load Model (Base or GalLoP-Trained)."""
    print(f"Loading Model: {args.model_id}")
    
    # 1. Load Base Model
    # Determine class based on model_id
    if "llava" in args.model_id.lower():
        from transformers import LlavaNextForConditionalGeneration
        model_cls = LlavaNextForConditionalGeneration
    else:
        from transformers import Qwen2_5_VLForConditionalGeneration
        model_cls = Qwen2_5_VLForConditionalGeneration
        
    model = model_cls.from_pretrained(
        args.model_id,
        device_map="auto",
        torch_dtype=torch.float16,
        trust_remote_code=True
    )
    processor = AutoProcessor.from_pretrained(args.model_id, trust_remote_code=True)
    
    # 2. Load GalLoP Adapter (if enabled)
    if args.use_gallop:
        print(f"Loading GalLoP Adapter from: {args.adapter_path}")
        if not os.path.exists(args.adapter_path):
            raise ValueError(f"Adapter path not found: {args.adapter_path}")
            
        model = PeftModel.from_pretrained(model, args.adapter_path)
    
    model.eval()
    return model, processor

def run_inference(model, processor, image, caption, claim, args):
    """Run inference for a single sample."""
    
    # Construct Prompt
    # Note: GalLoP training used specific format, we must match it during inference if using adapter.
    # Format used in training: "Context: {caption}\nClaim: {claim}\nVerify: "
    
    if "llava" in args.model_id.lower():
        # LLaVA Format
        prompt_text = f"[INST] <image>\nContext: {caption}\nClaim: {claim}\nVerify: [/INST]"
        inputs = processor(text=prompt_text, images=image, return_tensors="pt").to(model.device)
    else:
        # Qwen Format
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": image},
                    {"type": "text", "text": f"Context: {caption}\nClaim: {claim}\nVerify: "}
                ],
            }
        ]
        inputs = processor.apply_chat_template(
            messages, tokenize=True, add_generation_prompt=True, return_dict=True, return_tensors="pt"
        )
        inputs = inputs.to(model.device)
        
    with torch.no_grad():
        generated_ids = model.generate(**inputs, max_new_tokens=20)
        
    # Decode
    generated_ids_trimmed = [
        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
    ]
    output_text = processor.batch_decode(
        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
    )[0]
    
    return output_text.strip()

def normalize_label(text):
    text = text.upper()
    if "SUPPORT" in text: return "SUPPORTED"
    if "REFUT" in text: return "REFUTED"
    if "NEI" in text or "NOT ENOUGH" in text: return "NEI"
    return "NEI" # Default fallback

def run_benchmark(args):
    print(f"--- Benchmark Configuration ---")
    print(f"Model: {args.model_id}")
    print(f"GalLoP Trained: {args.use_gallop}")
    print(f"Localized Evidence: {args.use_localized}")
    
    # Load Model
    model, processor = load_model(args)
    
    # Load Data
    data = load_dataset_with_images()
    if args.limit > 0:
        data = data[:args.limit]
        
    predictions = []
    labels = []
    
    print("Starting Inference...")
    for sample in tqdm(data):
        # 1. Image Selection
        image_path = None
        
        if args.use_localized:
            # Try to find localized crop
            # Logic: We need to find the crop path generated by localize_panels.py
            # localize_panels.py saves as: {basename}_{panel_id}.jpg
            # But the dataloader might not know about it. 
            
            # Quick hack: Check if 'localized_features' dir exists and try to find matching file
            # If not found, fallback to full figure? Or skip?
            # Standard Step 6 "Localized" means we primarily use the crop.
            
            # For simplicity, we assume we want to use the crop if available.
            # But we need to know WHICH panel. The dataset has 'associated_figure_panels'.
            # We need to reconstruct the filename logic from localize_panels.py
            
            # Let's check if the sample has panel info
            panels = sample.get('associated_figure_panels', [])
            base_fname = os.path.basename(sample['associated_figure_filepath'])
            if base_fname.startswith("jacs_data"): base_fname = base_fname.replace("jacs_data_10.1021_", "chem_10.1021_")
            base_name_no_ext = os.path.splitext(base_fname)[0]
            
            # Try to find the crop file
            # Logic matches localize_panels.py output: {base_name}_{panel_id}.jpg
            # We need to know which panel ID corresponds to the claim. 
            # localize_panels.py extracts ALL panels involved.
            # If multiple panels, it saves multiple crops. 
            # We pick the first one for now? Or concatenated?
            # For this benchmark script, let's pick the first valid crop found.
            
            found_crop = False
            if panels:
                # Iterate and find first existing crop
                localized_dir = "localized_features" # Fixed path
                # We need to re-implement extract_panel_id logic ideally, or match loosely
                candidates = [str(p).strip().upper().replace(" ", "_") for p in panels]
                for p_id in candidates:
                    crop_fname = f"{base_name_no_ext}_{p_id}.jpg"
                    crop_path = os.path.join(localized_dir, crop_fname)
                    if os.path.exists(crop_path):
                        image_path = crop_path
                        found_crop = True
                        break
            
            if not found_crop:
                # If "Localized" is strictly requested but not found, we fallback to full figure?
                # Usually better to fallback than crash, but note it.
                # Construct full figure path
                image_path = os.path.join(PAPER_FIGURES_DIR, base_fname)
        
        else:
            # Full Figure (GalLoP Only or Base)
            base_fname = os.path.basename(sample['associated_figure_filepath'])
            if base_fname.startswith("jacs_data"): base_fname = base_fname.replace("jacs_data_10.1021_", "chem_10.1021_")
            image_path = os.path.join(PAPER_FIGURES_DIR, base_fname)
            
        if not os.path.exists(image_path):
            continue
            
        try:
            image = Image.open(image_path).convert("RGB")
        except:
            continue
            
        # 2. Inference
        response = run_inference(
            model, 
            processor, 
            image, 
            caption=sample.get('caption', '') or sample.get('figure_caption', ''), 
            claim=sample['claim_text'],
            args=args
        )
        
        pred = normalize_label(response)
        truth = normalize_label(sample['label_3class'])
        
        predictions.append(pred)
        labels.append(truth)
        
    # Metrics
    metrics = calculate_metrics(predictions, labels)
    print_metrics(metrics, f"Results (GalLoP={args.use_gallop}, Localized={args.use_localized})")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--model_id", type=str, default="Qwen/Qwen2.5-VL-7B-Instruct")
    parser.add_argument("--use_gallop", action="store_true", help="Use GalLoP trained adapter")
    parser.add_argument("--adapter_path", type=str, default="gallop_checkpoints", help="Path to adapter")
    parser.add_argument("--use_localized", action="store_true", help="Use localized/cropped images")
    parser.add_argument("--limit", type=int, default=0)
    args = parser.parse_args()
    
    run_benchmark(args)
